{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hybrid IDS: Flow-Based Binary Detection + Multiclass Attack Classification\n",
                "\n",
                "This notebook implements a two-stage intrusion detection system:\n",
                "1. **Stage 1 (Binary)**: Ensemble model to classify traffic as Attack vs Benign\n",
                "2. **Stage 2 (Multiclass)**: For detected attacks, classify the attack type using SPLT features\n",
                "\n",
                "## Features\n",
                "- Domain-invariant features for robustness across different networks\n",
                "- Calibrated confidence thresholds to reduce false positives\n",
                "- SPLT (Sequence of Packet Length and Time) for attack type classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ML imports\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.calibration import CalibratedClassifierCV\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, roc_auc_score, \n",
                "    precision_recall_curve, roc_curve, f1_score, accuracy_score\n",
                ")\n",
                "import joblib\n",
                "\n",
                "# Optional: XGBoost and LightGBM\n",
                "try:\n",
                "    import xgboost as xgb\n",
                "    HAS_XGB = True\n",
                "except ImportError:\n",
                "    HAS_XGB = False\n",
                "    print(\"XGBoost not installed, will use alternatives\")\n",
                "\n",
                "try:\n",
                "    import lightgbm as lgb\n",
                "    HAS_LGB = True\n",
                "except ImportError:\n",
                "    HAS_LGB = False\n",
                "    print(\"LightGBM not installed, will use alternatives\")\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"✓ Imports complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === CONFIGURATION ===\n",
                "# Modify these paths based on your setup\n",
                "\n",
                "DATA_DIR = Path(\"../data\")\n",
                "OUTPUT_DIR = Path(\"../outputs/hybrid_ids\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# CICIDS2017 labeled data\n",
                "# Download from: https://www.unb.ca/cic/datasets/ids-2017.html\n",
                "CICIDS_CSV_DIR = DATA_DIR / \"cicids2017\"  # Directory with labeled CSVs\n",
                "\n",
                "# T-Pot attack flows (extracted with DPI)\n",
                "ATTACK_FLOWS_PATH = DATA_DIR / \"attack_flows_dpi.csv\"\n",
                "\n",
                "# Benign flows (from CICIDS Monday or extracted from benign PCAP)\n",
                "BENIGN_FLOWS_PATH = DATA_DIR / \"benign_flows_dpi.csv\"\n",
                "\n",
                "# Detection thresholds (can be tuned per environment)\n",
                "CONFIG = {\n",
                "    \"binary_threshold\": 0.7,      # Confidence threshold for binary detection\n",
                "    \"multiclass_threshold\": 0.5,  # Confidence threshold for attack type\n",
                "    \"test_size\": 0.2,\n",
                "    \"random_state\": 42,\n",
                "}\n",
                "\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading & Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_cicids_data(csv_dir: Path, sample_frac: float = 1.0) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load CICIDS2017 labeled CSV files.\n",
                "    \"\"\"\n",
                "    all_dfs = []\n",
                "    csv_files = list(csv_dir.glob(\"*.csv\"))\n",
                "    \n",
                "    if not csv_files:\n",
                "        raise FileNotFoundError(f\"No CSV files found in {csv_dir}\")\n",
                "    \n",
                "    for csv_file in csv_files:\n",
                "        print(f\"Loading {csv_file.name}...\")\n",
                "        try:\n",
                "            df = pd.read_csv(csv_file, encoding='utf-8', low_memory=False)\n",
                "        except UnicodeDecodeError:\n",
                "            df = pd.read_csv(csv_file, encoding='latin-1', low_memory=False)\n",
                "        \n",
                "        # Standardize column names\n",
                "        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
                "        \n",
                "        if sample_frac < 1.0:\n",
                "            df = df.sample(frac=sample_frac, random_state=42)\n",
                "        \n",
                "        all_dfs.append(df)\n",
                "        print(f\"  → {len(df):,} rows, {len(df.columns)} columns\")\n",
                "    \n",
                "    combined = pd.concat(all_dfs, ignore_index=True)\n",
                "    print(f\"\\nTotal: {len(combined):,} rows\")\n",
                "    return combined\n",
                "\n",
                "\n",
                "def load_extracted_flows(path: Path) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Load flows extracted with our DPI script.\n",
                "    \"\"\"\n",
                "    if not path.exists():\n",
                "        raise FileNotFoundError(f\"Flow file not found: {path}\")\n",
                "    \n",
                "    df = pd.read_csv(path)\n",
                "    print(f\"Loaded {len(df):,} flows from {path.name}\")\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load CICIDS2017 data if available\n",
                "USE_CICIDS = CICIDS_CSV_DIR.exists()\n",
                "\n",
                "if USE_CICIDS:\n",
                "    print(\"Loading CICIDS2017 dataset...\")\n",
                "    cicids_df = load_cicids_data(CICIDS_CSV_DIR, sample_frac=0.5)  # Sample for speed\n",
                "    \n",
                "    # Show label distribution\n",
                "    if 'label' in cicids_df.columns:\n",
                "        print(\"\\nLabel distribution:\")\n",
                "        print(cicids_df['label'].value_counts())\n",
                "else:\n",
                "    print(f\"CICIDS2017 directory not found: {CICIDS_CSV_DIR}\")\n",
                "    print(\"Will use extracted flows from PCAPs instead.\")\n",
                "    cicids_df = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load extracted flows (if using our DPI extraction)\n",
                "USE_EXTRACTED = ATTACK_FLOWS_PATH.exists() and BENIGN_FLOWS_PATH.exists()\n",
                "\n",
                "if USE_EXTRACTED:\n",
                "    print(\"Loading extracted flows...\")\n",
                "    attack_flows = load_extracted_flows(ATTACK_FLOWS_PATH)\n",
                "    benign_flows = load_extracted_flows(BENIGN_FLOWS_PATH)\n",
                "    \n",
                "    # Add labels\n",
                "    attack_flows['label'] = 'attack'\n",
                "    benign_flows['label'] = 'benign'\n",
                "    \n",
                "    # Combine\n",
                "    extracted_df = pd.concat([attack_flows, benign_flows], ignore_index=True)\n",
                "    print(f\"\\nCombined: {len(extracted_df):,} flows\")\n",
                "else:\n",
                "    print(\"Extracted flows not found. Run extract_flows_with_dpi.py first:\")\n",
                "    print(f\"  python src/extract_flows_with_dpi.py --pcap <attack.pcap> --output {ATTACK_FLOWS_PATH}\")\n",
                "    print(f\"  python src/extract_flows_with_dpi.py --pcap <benign.pcap> --output {BENIGN_FLOWS_PATH}\")\n",
                "    extracted_df = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Choose which dataset to use\n",
                "if cicids_df is not None:\n",
                "    df = cicids_df\n",
                "    print(\"Using CICIDS2017 dataset\")\n",
                "elif extracted_df is not None:\n",
                "    df = extracted_df\n",
                "    print(\"Using extracted flows\")\n",
                "else:\n",
                "    raise RuntimeError(\"No dataset available! Please provide CICIDS2017 data or extract flows from PCAPs.\")\n",
                "\n",
                "print(f\"\\nDataset shape: {df.shape}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Feature Engineering (Domain-Invariant)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_binary_label(labels: pd.Series) -> pd.Series:\n",
                "    \"\"\"\n",
                "    Convert multiclass labels to binary (benign=0, attack=1).\n",
                "    \"\"\"\n",
                "    # CICIDS2017 uses 'BENIGN' for normal traffic\n",
                "    binary = labels.str.upper().apply(\n",
                "        lambda x: 0 if 'BENIGN' in str(x).upper() else 1\n",
                "    )\n",
                "    return binary\n",
                "\n",
                "\n",
                "def engineer_domain_invariant_features(df: pd.DataFrame) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Create domain-invariant features that transfer across networks.\n",
                "    \"\"\"\n",
                "    df = df.copy()\n",
                "    \n",
                "    # Find common flow columns (works with both CICIDS and our extracted data)\n",
                "    col_map = {\n",
                "        # CICIDS names -> our names\n",
                "        'flow_duration': 'duration',\n",
                "        'total_fwd_packets': 'fwd_packets',\n",
                "        'total_backward_packets': 'bwd_packets',\n",
                "        'total_length_of_fwd_packets': 'fwd_bytes',\n",
                "        'total_length_of_bwd_packets': 'bwd_bytes',\n",
                "        'fwd_packet_length_mean': 'fwd_pkt_mean',\n",
                "        'bwd_packet_length_mean': 'bwd_pkt_mean',\n",
                "        'flow_iat_mean': 'iat_mean',\n",
                "        'flow_iat_std': 'iat_std',\n",
                "        'fwd_iat_mean': 'fwd_iat_mean',\n",
                "        'bwd_iat_mean': 'bwd_iat_mean',\n",
                "        'syn_flag_count': 'syn_count',\n",
                "        'ack_flag_count': 'ack_count',\n",
                "        'psh_flag_count': 'psh_count',\n",
                "        'rst_flag_count': 'rst_count',\n",
                "    }\n",
                "    \n",
                "    # Rename if CICIDS format\n",
                "    for old, new in col_map.items():\n",
                "        if old in df.columns and new not in df.columns:\n",
                "            df[new] = df[old]\n",
                "    \n",
                "    # === Create ratio features (network-invariant) ===\n",
                "    if 'fwd_packets' in df.columns and 'bwd_packets' in df.columns:\n",
                "        total_pkts = df['fwd_packets'] + df['bwd_packets'] + 1\n",
                "        df['fwd_pkt_ratio'] = df['fwd_packets'] / total_pkts\n",
                "        df['bwd_pkt_ratio'] = df['bwd_packets'] / total_pkts\n",
                "    \n",
                "    if 'fwd_bytes' in df.columns and 'bwd_bytes' in df.columns:\n",
                "        total_bytes = df['fwd_bytes'] + df['bwd_bytes'] + 1\n",
                "        df['fwd_bytes_ratio'] = df['fwd_bytes'] / total_bytes\n",
                "        df['bwd_bytes_ratio'] = df['bwd_bytes'] / total_bytes\n",
                "    \n",
                "    # === Log-transform volume features ===\n",
                "    volume_cols = ['fwd_packets', 'bwd_packets', 'fwd_bytes', 'bwd_bytes', 'duration']\n",
                "    for col in volume_cols:\n",
                "        if col in df.columns:\n",
                "            df[f'log_{col}'] = np.log1p(df[col].clip(lower=0))\n",
                "    \n",
                "    # === Packet size ratios ===\n",
                "    if 'fwd_pkt_mean' in df.columns and 'bwd_pkt_mean' in df.columns:\n",
                "        df['pkt_size_ratio'] = df['fwd_pkt_mean'] / (df['bwd_pkt_mean'] + 1)\n",
                "    \n",
                "    # === IAT ratios ===\n",
                "    if 'fwd_iat_mean' in df.columns and 'bwd_iat_mean' in df.columns:\n",
                "        df['iat_ratio'] = df['fwd_iat_mean'] / (df['bwd_iat_mean'] + 1)\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply feature engineering\n",
                "df = engineer_domain_invariant_features(df)\n",
                "\n",
                "# Create labels\n",
                "if 'label' in df.columns:\n",
                "    df['binary_label'] = create_binary_label(df['label'])\n",
                "    df['attack_type'] = df['label'].where(df['binary_label'] == 1, 'BENIGN')\n",
                "    \n",
                "    print(\"Binary label distribution:\")\n",
                "    print(df['binary_label'].value_counts())\n",
                "    print(f\"\\nAttack ratio: {df['binary_label'].mean():.2%}\")\n",
                "else:\n",
                "    raise ValueError(\"No 'label' column found in dataset!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select features for training\n",
                "# Exclude identifiers and raw values that don't transfer across networks\n",
                "EXCLUDE_COLS = [\n",
                "    'label', 'binary_label', 'attack_type',  # Labels\n",
                "    'src_ip', 'dst_ip', 'source_ip', 'destination_ip',  # IPs\n",
                "    'src_port', 'dst_port', 'source_port', 'destination_port',  # Raw ports\n",
                "    'flow_id', 'timestamp', 'first_seen_ms', 'last_seen_ms',  # Identifiers\n",
                "    'requested_server_name', 'client_fingerprint',  # Strings\n",
                "    'application_name', 'application_category',  # Will one-hot encode separately\n",
                "]\n",
                "\n",
                "# Get numeric columns only\n",
                "feature_cols = [\n",
                "    col for col in df.columns \n",
                "    if col not in EXCLUDE_COLS \n",
                "    and df[col].dtype in ['int64', 'float64', 'int32', 'float32']\n",
                "]\n",
                "\n",
                "print(f\"Selected {len(feature_cols)} features for training\")\n",
                "print(f\"Features: {feature_cols[:20]}...\")  # Show first 20"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare feature matrix\n",
                "X = df[feature_cols].copy()\n",
                "\n",
                "# Handle missing/infinite values\n",
                "X = X.replace([np.inf, -np.inf], np.nan)\n",
                "X = X.fillna(X.median())\n",
                "\n",
                "# Binary labels\n",
                "y_binary = df['binary_label'].values\n",
                "\n",
                "# Multiclass labels (for attacks only)\n",
                "attack_mask = df['binary_label'] == 1\n",
                "y_multiclass = df.loc[attack_mask, 'attack_type'].values\n",
                "\n",
                "print(f\"Feature matrix shape: {X.shape}\")\n",
                "print(f\"Binary labels: {len(y_binary)} (benign={sum(y_binary==0)}, attack={sum(y_binary==1)})\")\n",
                "print(f\"Multiclass labels: {len(y_multiclass)} attacks\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Stage 1: Binary Classification (Attack vs Benign)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y_binary, \n",
                "    test_size=CONFIG['test_size'], \n",
                "    random_state=CONFIG['random_state'],\n",
                "    stratify=y_binary\n",
                ")\n",
                "\n",
                "print(f\"Train: {X_train.shape[0]:,} samples\")\n",
                "print(f\"Test:  {X_test.shape[0]:,} samples\")\n",
                "\n",
                "# Scale features\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build ensemble classifiers\n",
                "print(\"Training base classifiers...\")\n",
                "\n",
                "# Random Forest\n",
                "rf = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=15,\n",
                "    min_samples_leaf=5,\n",
                "    n_jobs=-1,\n",
                "    random_state=CONFIG['random_state']\n",
                ")\n",
                "\n",
                "# Gradient Boosting\n",
                "gb = GradientBoostingClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    random_state=CONFIG['random_state']\n",
                ")\n",
                "\n",
                "classifiers = [('rf', rf), ('gb', gb)]\n",
                "\n",
                "# Add XGBoost if available\n",
                "if HAS_XGB:\n",
                "    xgb_clf = xgb.XGBClassifier(\n",
                "        n_estimators=100,\n",
                "        max_depth=6,\n",
                "        learning_rate=0.1,\n",
                "        n_jobs=-1,\n",
                "        random_state=CONFIG['random_state'],\n",
                "        use_label_encoder=False,\n",
                "        eval_metric='logloss'\n",
                "    )\n",
                "    classifiers.append(('xgb', xgb_clf))\n",
                "\n",
                "# Add LightGBM if available\n",
                "if HAS_LGB:\n",
                "    lgb_clf = lgb.LGBMClassifier(\n",
                "        n_estimators=100,\n",
                "        max_depth=6,\n",
                "        learning_rate=0.1,\n",
                "        n_jobs=-1,\n",
                "        random_state=CONFIG['random_state'],\n",
                "        verbose=-1\n",
                "    )\n",
                "    classifiers.append(('lgb', lgb_clf))\n",
                "\n",
                "print(f\"Ensemble: {[name for name, _ in classifiers]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train voting ensemble\n",
                "ensemble = VotingClassifier(\n",
                "    estimators=classifiers,\n",
                "    voting='soft'  # Use probabilities for confidence\n",
                ")\n",
                "\n",
                "print(\"Training ensemble...\")\n",
                "ensemble.fit(X_train_scaled, y_train)\n",
                "print(\"✓ Training complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calibrate probabilities for better confidence estimates\n",
                "print(\"Calibrating probabilities...\")\n",
                "calibrated_ensemble = CalibratedClassifierCV(ensemble, cv=3, method='isotonic')\n",
                "calibrated_ensemble.fit(X_train_scaled, y_train)\n",
                "print(\"✓ Calibration complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict with calibrated probabilities\n",
                "y_proba = calibrated_ensemble.predict_proba(X_test_scaled)[:, 1]\n",
                "y_pred = (y_proba >= CONFIG['binary_threshold']).astype(int)\n",
                "\n",
                "# Also get predictions with default threshold for comparison\n",
                "y_pred_default = calibrated_ensemble.predict(X_test_scaled)\n",
                "\n",
                "print(f\"Threshold: {CONFIG['binary_threshold']}\")\n",
                "print(f\"\\n=== With Default Threshold (0.5) ===\")\n",
                "print(classification_report(y_test, y_pred_default, target_names=['Benign', 'Attack']))\n",
                "\n",
                "print(f\"\\n=== With Custom Threshold ({CONFIG['binary_threshold']}) ===\")\n",
                "print(classification_report(y_test, y_pred, target_names=['Benign', 'Attack']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ROC curve\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# ROC Curve\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
                "auc = roc_auc_score(y_test, y_proba)\n",
                "\n",
                "axes[0].plot(fpr, tpr, label=f'ROC (AUC = {auc:.3f})', linewidth=2)\n",
                "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "axes[0].axvline(x=fpr[np.argmin(np.abs(thresholds - CONFIG['binary_threshold']))], \n",
                "                color='r', linestyle=':', label=f\"Threshold={CONFIG['binary_threshold']}\")\n",
                "axes[0].set_xlabel('False Positive Rate')\n",
                "axes[0].set_ylabel('True Positive Rate')\n",
                "axes[0].set_title('ROC Curve - Binary Detection')\n",
                "axes[0].legend()\n",
                "\n",
                "# Precision-Recall Curve\n",
                "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
                "axes[1].plot(recall, precision, linewidth=2)\n",
                "axes[1].set_xlabel('Recall')\n",
                "axes[1].set_ylabel('Precision')\n",
                "axes[1].set_title('Precision-Recall Curve')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'binary_roc_pr_curves.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "for ax, pred, title in [\n",
                "    (axes[0], y_pred_default, 'Default Threshold (0.5)'),\n",
                "    (axes[1], y_pred, f'Custom Threshold ({CONFIG[\"binary_threshold\"]})')\n",
                "]:\n",
                "    cm = confusion_matrix(y_test, pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
                "                xticklabels=['Benign', 'Attack'],\n",
                "                yticklabels=['Benign', 'Attack'])\n",
                "    ax.set_xlabel('Predicted')\n",
                "    ax.set_ylabel('Actual')\n",
                "    ax.set_title(title)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'binary_confusion_matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Stage 2: Multiclass Attack Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare multiclass data (attacks only)\n",
                "X_attacks = X[attack_mask].copy()\n",
                "y_attacks = y_multiclass\n",
                "\n",
                "# Encode attack types\n",
                "label_encoder = LabelEncoder()\n",
                "y_attacks_encoded = label_encoder.fit_transform(y_attacks)\n",
                "\n",
                "print(f\"Attack samples: {len(X_attacks):,}\")\n",
                "print(f\"\\nAttack type distribution:\")\n",
                "for label, count in zip(*np.unique(y_attacks, return_counts=True)):\n",
                "    print(f\"  {label}: {count:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train/test split for multiclass\n",
                "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(\n",
                "    X_attacks, y_attacks_encoded,\n",
                "    test_size=CONFIG['test_size'],\n",
                "    random_state=CONFIG['random_state'],\n",
                "    stratify=y_attacks_encoded\n",
                ")\n",
                "\n",
                "# Scale\n",
                "scaler_mc = StandardScaler()\n",
                "X_train_mc_scaled = scaler_mc.fit_transform(X_train_mc)\n",
                "X_test_mc_scaled = scaler_mc.transform(X_test_mc)\n",
                "\n",
                "print(f\"Multiclass Train: {X_train_mc.shape[0]:,}\")\n",
                "print(f\"Multiclass Test:  {X_test_mc.shape[0]:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train multiclass classifier\n",
                "print(\"Training multiclass classifier...\")\n",
                "\n",
                "if HAS_XGB:\n",
                "    mc_classifier = xgb.XGBClassifier(\n",
                "        n_estimators=150,\n",
                "        max_depth=8,\n",
                "        learning_rate=0.1,\n",
                "        n_jobs=-1,\n",
                "        random_state=CONFIG['random_state'],\n",
                "        use_label_encoder=False,\n",
                "        eval_metric='mlogloss'\n",
                "    )\n",
                "else:\n",
                "    mc_classifier = RandomForestClassifier(\n",
                "        n_estimators=150,\n",
                "        max_depth=15,\n",
                "        n_jobs=-1,\n",
                "        random_state=CONFIG['random_state']\n",
                "    )\n",
                "\n",
                "mc_classifier.fit(X_train_mc_scaled, y_train_mc)\n",
                "print(\"✓ Training complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate multiclass\n",
                "y_pred_mc = mc_classifier.predict(X_test_mc_scaled)\n",
                "\n",
                "print(\"Multiclass Classification Report:\")\n",
                "print(classification_report(\n",
                "    y_test_mc, y_pred_mc, \n",
                "    target_names=label_encoder.classes_\n",
                "))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Multiclass confusion matrix\n",
                "cm_mc = confusion_matrix(y_test_mc, y_pred_mc)\n",
                "\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(cm_mc, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=label_encoder.classes_,\n",
                "            yticklabels=label_encoder.classes_)\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title('Multiclass Attack Type Classification')\n",
                "plt.xticks(rotation=45, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'multiclass_confusion_matrix.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Robustness: Adversarial Validation & Drift Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def adversarial_validation(X_train: np.ndarray, X_test: np.ndarray) -> float:\n",
                "    \"\"\"\n",
                "    Detect domain shift by training a classifier to distinguish train vs test.\n",
                "    AUC > 0.6 indicates significant domain shift.\n",
                "    \"\"\"\n",
                "    # Create labels: train=0, test=1\n",
                "    y_adv = np.concatenate([np.zeros(len(X_train)), np.ones(len(X_test))])\n",
                "    X_adv = np.vstack([X_train, X_test])\n",
                "    \n",
                "    # Train quick classifier\n",
                "    adv_clf = RandomForestClassifier(n_estimators=50, max_depth=5, n_jobs=-1, random_state=42)\n",
                "    \n",
                "    # Cross-validated AUC\n",
                "    cv_scores = cross_val_score(adv_clf, X_adv, y_adv, cv=3, scoring='roc_auc')\n",
                "    mean_auc = cv_scores.mean()\n",
                "    \n",
                "    return mean_auc\n",
                "\n",
                "\n",
                "# Check for domain shift between train and test\n",
                "adv_auc = adversarial_validation(X_train_scaled, X_test_scaled)\n",
                "\n",
                "print(f\"Adversarial Validation AUC: {adv_auc:.3f}\")\n",
                "if adv_auc > 0.6:\n",
                "    print(\"⚠️  WARNING: Significant domain shift detected!\")\n",
                "    print(\"   Consider collecting more diverse training data.\")\n",
                "else:\n",
                "    print(\"✓ No significant domain shift detected.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def threshold_sensitivity_analysis(y_true, y_proba, thresholds=None):\n",
                "    \"\"\"\n",
                "    Analyze how different thresholds affect FP/TP rates.\n",
                "    \"\"\"\n",
                "    if thresholds is None:\n",
                "        thresholds = np.arange(0.3, 0.95, 0.05)\n",
                "    \n",
                "    results = []\n",
                "    for thresh in thresholds:\n",
                "        y_pred = (y_proba >= thresh).astype(int)\n",
                "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
                "        \n",
                "        results.append({\n",
                "            'threshold': thresh,\n",
                "            'precision': tp / (tp + fp + 1e-9),\n",
                "            'recall': tp / (tp + fn + 1e-9),\n",
                "            'fpr': fp / (fp + tn + 1e-9),\n",
                "            'f1': f1_score(y_true, y_pred),\n",
                "        })\n",
                "    \n",
                "    return pd.DataFrame(results)\n",
                "\n",
                "\n",
                "# Threshold sensitivity\n",
                "thresh_df = threshold_sensitivity_analysis(y_test, y_proba)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.plot(thresh_df['threshold'], thresh_df['precision'], label='Precision', marker='o')\n",
                "ax.plot(thresh_df['threshold'], thresh_df['recall'], label='Recall', marker='s')\n",
                "ax.plot(thresh_df['threshold'], thresh_df['f1'], label='F1', marker='^')\n",
                "ax.plot(thresh_df['threshold'], thresh_df['fpr'], label='False Positive Rate', marker='x')\n",
                "ax.axvline(x=CONFIG['binary_threshold'], color='r', linestyle='--', \n",
                "           label=f\"Current Threshold ({CONFIG['binary_threshold']})\")\n",
                "ax.set_xlabel('Detection Threshold')\n",
                "ax.set_ylabel('Score')\n",
                "ax.set_title('Threshold Sensitivity Analysis')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(OUTPUT_DIR / 'threshold_sensitivity.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nThreshold Analysis:\")\n",
                "print(thresh_df.round(3).to_string(index=False))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save all artifacts\n",
                "models_dir = OUTPUT_DIR / 'models'\n",
                "models_dir.mkdir(exist_ok=True)\n",
                "\n",
                "# Save models\n",
                "joblib.dump(calibrated_ensemble, models_dir / 'binary_classifier.joblib')\n",
                "joblib.dump(mc_classifier, models_dir / 'multiclass_classifier.joblib')\n",
                "joblib.dump(scaler, models_dir / 'binary_scaler.joblib')\n",
                "joblib.dump(scaler_mc, models_dir / 'multiclass_scaler.joblib')\n",
                "joblib.dump(label_encoder, models_dir / 'label_encoder.joblib')\n",
                "\n",
                "# Save config\n",
                "import json\n",
                "with open(models_dir / 'config.json', 'w') as f:\n",
                "    json.dump({\n",
                "        **CONFIG,\n",
                "        'feature_columns': feature_cols,\n",
                "        'attack_types': label_encoder.classes_.tolist(),\n",
                "    }, f, indent=2)\n",
                "\n",
                "print(f\"✓ Models saved to {models_dir}\")\n",
                "print(f\"\\nFiles:\")\n",
                "for f in models_dir.iterdir():\n",
                "    print(f\"  - {f.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Inference Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_hybrid(flow_features: pd.DataFrame, \n",
                "                   binary_threshold: float = 0.7,\n",
                "                   multiclass_threshold: float = 0.5) -> pd.DataFrame:\n",
                "    \"\"\"\n",
                "    Two-stage hybrid prediction:\n",
                "    1. Binary: Is this traffic an attack?\n",
                "    2. Multiclass: If attack, what type?\n",
                "    \n",
                "    Args:\n",
                "        flow_features: DataFrame with flow features\n",
                "        binary_threshold: Confidence threshold for attack detection\n",
                "        multiclass_threshold: Confidence threshold for attack type\n",
                "    \n",
                "    Returns:\n",
                "        DataFrame with predictions\n",
                "    \"\"\"\n",
                "    # Load models (in production, load once at startup)\n",
                "    binary_clf = joblib.load(models_dir / 'binary_classifier.joblib')\n",
                "    mc_clf = joblib.load(models_dir / 'multiclass_classifier.joblib')\n",
                "    binary_scaler = joblib.load(models_dir / 'binary_scaler.joblib')\n",
                "    mc_scaler = joblib.load(models_dir / 'multiclass_scaler.joblib')\n",
                "    le = joblib.load(models_dir / 'label_encoder.joblib')\n",
                "    \n",
                "    # Prepare features\n",
                "    X = flow_features[feature_cols].copy()\n",
                "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
                "    \n",
                "    # Stage 1: Binary detection\n",
                "    X_scaled = binary_scaler.transform(X)\n",
                "    attack_proba = binary_clf.predict_proba(X_scaled)[:, 1]\n",
                "    is_attack = (attack_proba >= binary_threshold).astype(int)\n",
                "    \n",
                "    # Stage 2: Multiclass (only for detected attacks)\n",
                "    attack_type = np.full(len(X), 'BENIGN', dtype=object)\n",
                "    attack_type_proba = np.zeros(len(X))\n",
                "    \n",
                "    attack_mask = is_attack == 1\n",
                "    if attack_mask.any():\n",
                "        X_attacks = mc_scaler.transform(X[attack_mask])\n",
                "        mc_proba = mc_clf.predict_proba(X_attacks)\n",
                "        mc_pred = mc_clf.predict(X_attacks)\n",
                "        mc_confidence = mc_proba.max(axis=1)\n",
                "        \n",
                "        # Apply multiclass threshold\n",
                "        confident_preds = mc_confidence >= multiclass_threshold\n",
                "        attack_type[attack_mask] = np.where(\n",
                "            confident_preds,\n",
                "            le.inverse_transform(mc_pred),\n",
                "            'UNKNOWN_ATTACK'\n",
                "        )\n",
                "        attack_type_proba[attack_mask] = mc_confidence\n",
                "    \n",
                "    # Create results DataFrame\n",
                "    results = pd.DataFrame({\n",
                "        'is_attack': is_attack,\n",
                "        'attack_confidence': attack_proba,\n",
                "        'attack_type': attack_type,\n",
                "        'type_confidence': attack_type_proba,\n",
                "    })\n",
                "    \n",
                "    return results\n",
                "\n",
                "\n",
                "# Test inference\n",
                "print(\"Testing inference function...\")\n",
                "sample = X.iloc[:10].copy()\n",
                "results = predict_hybrid(df.iloc[:10], binary_threshold=0.7)\n",
                "print(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Binary Classification Results\n",
                "- **Model**: Calibrated ensemble (RF + GB + XGBoost + LightGBM)\n",
                "- **Threshold**: Adjustable (default 0.7 for low false positives)\n",
                "\n",
                "### Multiclass Classification Results  \n",
                "- **Model**: XGBoost (or RF fallback)\n",
                "- **Classes**: Attack types from CICIDS2017\n",
                "\n",
                "### Robustness Features\n",
                "- Domain-invariant features (ratios, log transforms)\n",
                "- Calibrated confidence scores\n",
                "- Adjustable thresholds per environment\n",
                "- Adversarial validation for drift detection"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}